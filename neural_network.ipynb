{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually still more data stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"MNIST.csv\")\n",
    "data = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df.head())\n",
    "# print(data[0])\n",
    "\n",
    "# for i in range(0, 5):\n",
    "#     pixels = data[i][:784].reshape(28,28)\n",
    "#     plt.imshow(pixels, cmap='gray')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, train_proportion = 0.8):\n",
    "    np.random.shuffle(data)\n",
    "    m, n = data.shape\n",
    "\n",
    "    train_size = math.floor(train_proportion * m)\n",
    "\n",
    "    train = data[:train_size]\n",
    "    test = data[train_size:m]\n",
    "\n",
    "    X_train = train.T[:n-1].T\n",
    "    X_train = X_train / np.max(X_train)\n",
    "    y_train = train.T[n-1].T\n",
    "\n",
    "    X_test = test.T[:n-1].T\n",
    "    X_test = X_test / np.max(X_train)\n",
    "    y_test = test.T[n-1].T\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = split(data, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Neural Network Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier:\n",
    "    def __init__(self, layers = (20,)) -> None:\n",
    "        self.layers = layers\n",
    "        self.attributes = []\n",
    "        self.attr_z = []\n",
    "        for i in layers:\n",
    "            self.attributes.append(np.random.rand(1, i) - .5)\n",
    "            self.attr_z.append(np.random.rand(1, i) - .5)\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for i in range(1, len(layers)):\n",
    "            n = layers[i - 1]\n",
    "            m = layers[i]\n",
    "\n",
    "            self.weights.append(np.random.rand(m, n) - .5)\n",
    "            self.biases.append(np.random.rand(1, m) - .5)\n",
    "    \n",
    "    def ReLU(self, X):\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def softmax(self,Z):\n",
    "        Z -= np.max(Z.T, axis=0).T  # Subtract max value for numerical stability\n",
    "        A = np.exp(Z) / np.sum(np.exp(Z), axis=0)\n",
    "        return A\n",
    "        \n",
    "    def one_hot(self, Y):\n",
    "        Y = Y.T\n",
    "        n, = Y.shape\n",
    "        m, = np.unique(y_train).shape\n",
    "\n",
    "        y = np.zeros((n, m))\n",
    "\n",
    "        y[np.arange(n), Y] = 1\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def forward_prop(self, input):\n",
    "        a = input\n",
    "        for (a_index, w, b) in zip(range(len(self.attributes)), self.weights, self.biases):\n",
    "            z = np.dot(w, a.T).T + b\n",
    "            a = self.ReLU(z)\n",
    "            self.attributes[a_index] = a\n",
    "            self.attr_z[a_index] = z\n",
    "        \n",
    "        return np.apply_along_axis(self.softmax, 1, a)\n",
    "\n",
    "    def fit(self, X_train, y_train, iterations = 100, learning_rate = 0.01):\n",
    "        self.__init__(self.layers)\n",
    "\n",
    "        samples, n = X_train.shape\n",
    "        _, m = self.attributes[0].shape\n",
    "        self.weights.insert(0, np.random.rand(m, n) - .5)\n",
    "        self.biases.insert(0, np.random.rand(1, m) - .5)\n",
    "\n",
    "        m, = np.unique(y_train).shape\n",
    "        _, n = self.attributes[-1].shape\n",
    "        self.weights.append(np.random.rand(m, n) - .5)\n",
    "        self.biases.append(np.random.rand(1, m) - .5)\n",
    "        self.attributes.append(np.random.rand(1, m) - .5)\n",
    "        self.attr_z.append(np.random.rand(1, m) - .5)\n",
    "\n",
    "        self.gradient_descent(X_train, y_train, iterations, learning_rate)\n",
    "\n",
    "    def predict(self, input):\n",
    "        a = input\n",
    "        for (w, b) in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a.T).T + b\n",
    "            a = self.ReLU(z)\n",
    "                \n",
    "        p = np.apply_along_axis(self.softmax, 1, a)\n",
    "\n",
    "        return np.apply_along_axis(np.argmax, 1, p)\n",
    "    \n",
    "    def derivReLU(self, Z):\n",
    "        return Z > 0\n",
    "\n",
    "    # Backpropogation\n",
    "    def back_propagation(self, X_train, y_train, alpha=0.01):\n",
    "        self.forward_prop(X_train)\n",
    "\n",
    "        n, _ = self.attributes[0].shape\n",
    "        self.attributes.insert(0, X_train)\n",
    "\n",
    "        num_layers = len(self.attributes) - 1\n",
    "\n",
    "        y = self.one_hot(y_train)\n",
    "        delta_y = 2 * (self.attributes[-1] - y).T\n",
    "\n",
    "        for i in range(num_layers, 0, -1):\n",
    "            dW = np.dot(delta_y, self.attributes[i - 1]) / n\n",
    "            dB = np.apply_along_axis(np.sum, 1, delta_y) / n\n",
    "\n",
    "            W = self.weights[i-1]\n",
    "            b = self.biases[i-1]\n",
    "\n",
    "            self.weights[i-1] = W - alpha * dW\n",
    "            self.biases[i-1] = b - alpha * dB\n",
    "\n",
    "            delta_y = np.dot(W.T, delta_y) * self.derivReLU(self.attributes[i - 1]).T\n",
    "\n",
    "        \n",
    "        self.attributes.pop(0)\n",
    "        \n",
    "    # Calculate accuracy (to see if the NN is working)\n",
    "    def calc_accuracy(self, X, y):\n",
    "        return np.sum(y == self.predict(X)) / y.size\n",
    "\n",
    "\n",
    "    # Gradient descent\n",
    "    def gradient_descent(self, X_train, y_train, iterations=500, learning_rate=0.01):\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        for i in range(iterations):\n",
    "            self.back_propagation(X_train, y_train, learning_rate)\n",
    "\n",
    "\n",
    "            if (not i%10):\n",
    "                print(\"Accuracy for Iteration\", i, \"is:\", self.calc_accuracy(X_train, y_train))\n",
    "                print(\"----------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_nn = MLPClassifier((10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Accuracy for Iteration 0 is: 0.16892063492063492\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\AppData\\Local\\Temp\\ipykernel_12380\\3243471216.py:24: RuntimeWarning: invalid value encountered in subtract\n",
      "  Z -= np.max(Z.T, axis=0).T  # Subtract max value for numerical stability\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Iteration 10 is: 0.09895238095238096\n",
      "----------------------------------------------------------------\n",
      "Accuracy for Iteration 20 is: 0.09895238095238096\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmlp_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 64\u001b[0m, in \u001b[0;36mMLPClassifier.fit\u001b[1;34m(self, X_train, y_train, iterations, learning_rate)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m.5\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattr_z\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m.5\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 117\u001b[0m, in \u001b[0;36mMLPClassifier.gradient_descent\u001b[1;34m(self, X_train, y_train, iterations, learning_rate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mback_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy for Iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m, i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_accuracy(X_train, y_train))\n",
      "Cell \u001b[1;32mIn[7], line 81\u001b[0m, in \u001b[0;36mMLPClassifier.back_propagation\u001b[1;34m(self, X_train, y_train, alpha)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mback_propagation\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_train, y_train, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m):\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     n, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, X_train)\n",
      "Cell \u001b[1;32mIn[7], line 47\u001b[0m, in \u001b[0;36mMLPClassifier.forward_prop\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes[a_index] \u001b[38;5;241m=\u001b[39m a\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattr_z[a_index] \u001b[38;5;241m=\u001b[39m z\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\lib\\shape_base.py:402\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m buff[ind0] \u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m inds:\n\u001b[1;32m--> 402\u001b[0m     buff[ind] \u001b[38;5;241m=\u001b[39m asanyarray(func1d(inarr_view[ind], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, matrix):\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# wrap the array, to preserve subclasses\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     buff \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39m__array_wrap__(buff)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp_nn.fit(X_train, y_train, 100, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp_nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmlp_nn\u001b[49m\u001b[38;5;241m.\u001b[39mcalc_accuracy(X_test, y_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mlp_nn' is not defined"
     ]
    }
   ],
   "source": [
    "mlp_nn.calc_accuracy(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
